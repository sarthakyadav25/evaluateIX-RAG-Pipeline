# RAG Microservice API Documentation

This document provides detailed information about the API endpoints available in the RAG Microservice. It is designed for backend engineers integrating these APIs into their applications.

## Base URL
Assuming the service is running locally:
`http://localhost:8000`

## Endpoints

### 1. Ingest Content
**Endpoint:** `/ingest`
**Method:** `POST`
**Description:** Ingests a batch of content into the Vector Database (ChromaDB). It supports both binary files (PDF, DOCX, TXT) and text/URL sources.

#### Request Parameters (multipart/form-data)
| Field | Type | Required | Description |
|---|---|---|---|
| `test_id` | `string` | Yes | Unique identifier for the test/exam. Used for tenancy isolation in the vector DB. |
| `tenant_id` | `string` | Yes | Unique identifier for the tenant. |
| `metadata` | `string` (JSON) | No | JSON string of global metadata to be attached to all ingested documents. Default: `{}`. |
| `documents_json` | `string` (JSON) | No | JSON string representing a list of `DocumentSource` objects. Default: `[]`. |
| `files` | `List[File]` | No | List of binary files to upload. Supported formats: `.pdf`, `.docx`, `.doc`, `.txt`. |

**`DocumentSource` Object Structure (inside `documents_json`):**
```json
{
  "url": "string (optional)", // URL to download file from
  "text": "string (optional)", // Direct raw text content
  "file_type": "string (optional)" // Explicit file type (e.g., 'pdf') if URL lacks extension
}
```

#### Response Body (`IngestResponse`)
```json
{
  "status": "string", // "success" or "completed_with_errors"
  "message": "string", // Summary message
  "processed_count": "integer", // Number of items successfully processed
  "errors": ["string"] // List of error messages if any occurred
}
```

#### What it does (Logic Flow)
1.  **Metadata Parsing:** Parses the `metadata` JSON string and injects `test_id` and `tenant_id` into it.
2.  **JSON Sources Processing:** Iterates through `documents_json`.
    - If `text` is provided, it uses it directly.
    - If `url` is provided, it downloads the file, determines the extension, and extracts text.
    - Sends the extracted text to the processing pipeline (chunking, embedding, storage).
3.  **Binary Files Processing:** Iterates through uploaded `files`.
    - Determines file type by extension.
    - Extracts text from the file content.
    - Sends the extracted text to the processing pipeline.
4.  **Response:** Returns the count of processed items and any errors encountered.

---

### 2. Retrieve Context & Score Answer
**Endpoint:** `/retrieve`
**Method:** `POST`
**Description:** Retrieves relevant context from the vector database based on a candidate's answer and the question, then uses an LLM (Gemini) to score the answer against the retrieved context.

#### Request Body (`RetrieveRequest`)
```json
{
  "question": "string", // The question asked to the candidate
  "query": "string", // The candidate's answer/response
  "filters": {
    "test_id": "string" // REQUIRED: The test_id to filter context by
  },
  "top_k": "integer" // Optional. Number of context chunks to retrieve. Default: 3
}
```

#### Response Body (`RetrieveResponse`)
```json
{
  "results": [
    {
      "content": "string", // The text content of the retrieved chunk
      "score": "float", // Similarity score (higher is better)
      "metadata": {} // Metadata associated with the chunk
    }
  ],
  "answer": {
    // JSON object generated by LLM (Gemini)
    "overall_score": "integer",
    "breakdown": [
      {"criterion": "accuracy", "score": "integer", "max": 40},
      {"criterion": "completeness", "score": "integer", "max": 25},
      {"criterion": "relevance", "score": "integer", "max": 15},
      {"criterion": "reasoning", "score": "integer", "max": 10},
      {"criterion": "clarity", "score": "integer", "max": 5},
      {"criterion": "citations", "score": "integer", "max": 5}
    ],
    "confidence": "float",
    "pass": "boolean",
    "rationale": "string",
    "improvements": ["string"],
    "evidence": {
      "supporting_doc_ids": ["string"],
      "contradicting_doc_ids": ["string"],
      "unsupported_claims": [
        {"claim": "string", "suggested_penalty_points": "integer"}
      ]
    }
  }
}
```

#### What it does (Logic Flow)
1.  **Embedding:** Generates an embedding vector for the `query` (candidate's answer).
2.  **Vector Search:** Queries ChromaDB for the `top_k` most similar chunks, strictly filtering by `test_id`.
3.  **Context Formatting:** Formats the retrieved documents and calculates a relevance score.
4.  **LLM Evaluation:** Constructs a prompt containing the `question`, `candidate_answer`, and `retrieved_docs`.
    - Calls Gemini 2.5 Flash to evaluate the answer based on a specific rubric (Accuracy, Completeness, Relevance, etc.).
    - The LLM is instructed to treat `retrieved_docs` as ground truth.
5.  **Response:** Returns the retrieved chunks (`results`) and the structured evaluation (`answer`).

---

### 3. Generate Questions
**Endpoint:** `/generate-questions`
**Method:** `POST`
**Description:** Generates a set of interview questions based on *all* content ingested for a specific `test_id`.

#### Request Body (`QuestionGenerationRequest`)
```json
{
  "test_id": "string", // ID of the test to generate questions for
  "num_questions": "integer", // Optional. Number of questions to generate. Default: 5
  "difficulty": "string", // Optional. Difficulty level (e.g., "medium"). Default: "medium"
  "already_has": ["string"] // List of existing questions to avoid duplicates
}
```

#### Response Body (`QuestionGenerationResponse`)
```json
{
  "questions": [
    {
      "question_no": "integer",
      "content": "string" // The generated question text
    }
  ]
}
```

#### What it does (Logic Flow)
1.  **Fetch All Context:** Retrieves *all* document chunks from ChromaDB that match the provided `test_id`.
2.  **Context Assembly:** Concatenates all chunks into a single large context string.
3.  **Prompt Engineering:** Constructs a prompt for Gemini 2.5 Flash:
    - Instructs it to generate `num_questions` of `difficulty` level.
    - Provides the `already_has` list to prevent duplicates.
    - Feeds the `full_context` as the source material.
4.  **Generation & Parsing:** Calls the LLM and parses the returned JSON list of questions.
    - Includes a fallback mechanism to parse line-by-line if JSON parsing fails.
5.  **Response:** Returns the list of generated questions.

---

### 4. Health Check
**Endpoint:** `/health`
**Method:** `GET`
**Description:** Simple health check endpoint for monitoring.

#### Response Body
```json
{
  "status": "operational",
  "service": "rag-pipeline"
}
```
